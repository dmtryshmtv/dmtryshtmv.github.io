<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Dmitry Shemetov</title>
  <meta name="description" content="&quot;Welcome to my Internet.&quot;">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/itsrg/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Dmitry Shemetov" href="http://localhost:4000/feed.xml">

  

  
  <meta property="og:title" content="Dmitry Shemetov">
  <meta property="og:site_name" content="Dmitry Shemetov">
  <meta property="og:url" content="http://localhost:4000/itsrg/">
  <meta property="og:description" content="&quot;Welcome to my Internet.&quot;">
  
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Dmitry Shemetov">
  <meta name="twitter:description" content="&quot;Welcome to my Internet.&quot;">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700&display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <div class="wrapper">

    <a class="site-title" href="/">Dmitry Shemetov</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="https://github.com/dshemetov/">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title"></h1>
  </header>

  <div class="post-content">
    <h1 id="information-theory-and-statistics-reading-group-meetings-2018---2019">Information Theory and Statistics Reading Group Meetings (2018 - 2019)</h1>
<h2 id="winter-quarter">Winter Quarter</h2>
<ul>
  <li>[03/05/2019] Tong Yitang presents on Lecture 7<sup id="fnref:9"><a href="#fn:9" class="footnote">1</a></sup>. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.</li>
  <li>[02/26/2019] David Weber presents on Lecture 6<sup id="fnref:9:1"><a href="#fn:9" class="footnote">1</a></sup>. Topics include graphical models.</li>
  <li>[02/19/2019] David Weber and Dmitry Shemetov present on Lecture 6<sup id="fnref:9:2"><a href="#fn:9" class="footnote">1</a></sup>. Topics include mutual information estimators and applications of mutual information to machine learning.</li>
  <li>[02/12/2019] Dmitry Shemetov presents on Lecture 4 and 5<sup id="fnref:9:3"><a href="#fn:9" class="footnote">1</a></sup>. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm<sup id="fnref:11"><a href="#fn:11" class="footnote">2</a></sup>.</li>
  <li>[01/29/2019] Dmitry Shemetov presents on Lecture 3<sup id="fnref:9:4"><a href="#fn:9" class="footnote">1</a></sup>. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement<sup id="fnref:10"><a href="#fn:10" class="footnote">3</a></sup>.</li>
  <li>[01/22/2019] David Weber presents on the challenges of estimating mutual information<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup> <sup id="fnref:8"><a href="#fn:8" class="footnote">5</a></sup>.</li>
  <li>[01/15/2019] Cong Xu presents on Chapter 5 “Data Compression”<sup id="fnref:1"><a href="#fn:1" class="footnote">6</a></sup>.</li>
</ul>

<h2 id="fall-quarter">Fall Quarter</h2>
<ul>
  <li>[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton<sup id="fnref:6"><a href="#fn:6" class="footnote">7</a></sup>: the blowing up lemma<sup id="fnref:7"><a href="#fn:7" class="footnote">8</a></sup>, single-letter characterization, Ornstein’s copying lemma, and Talagrand’s theorem.</li>
  <li>[12/07/2018] David Weber lays the foundations with Chapter 8 “Differential Entropy”<sup id="fnref:1:1"><a href="#fn:1" class="footnote">6</a></sup> and then presents on “Estimating Mutual Information”<sup id="fnref:5:1"><a href="#fn:5" class="footnote">4</a></sup>.</li>
  <li>[11/29/2018] Yiqun Shao presents<sup id="fnref:4"><a href="#fn:4" class="footnote">9</a></sup>.</li>
  <li>[11/09/2018] Stephen Sheng presents<sup id="fnref:3"><a href="#fn:3" class="footnote">10</a></sup>.</li>
  <li>[11/02/2018] Group read of Chapter 11 “Information Theory and Statistics”<sup id="fnref:1:2"><a href="#fn:1" class="footnote">6</a></sup>.</li>
  <li>[10/26/2018] Group read of Chapter 6 “Gambling and Data Compression”<sup id="fnref:1:3"><a href="#fn:1" class="footnote">6</a></sup>.</li>
  <li>[10/19/2018] Dmitry Shemetov presents on Chapter 2 “Lower bounds on minimax risk”<sup id="fnref:2"><a href="#fn:2" class="footnote">11</a></sup>.</li>
  <li>[10/12/2018] A problem solving session dedicated to Chapter 2<sup id="fnref:1:4"><a href="#fn:1" class="footnote">6</a></sup>.</li>
  <li>[10/05/2018] Dmitry Shemetov presents the foundational material from Chapter 2 “Entropy, Relative Entropy, Mutual Information”<sup id="fnref:1:5"><a href="#fn:1" class="footnote">6</a></sup>.</li>
  <li>[9/28/2018] Organizing meeting.</li>
</ul>

<h2 id="references">References</h2>
<div class="footnotes">
  <ol>
    <li id="fn:9">
      <p>A. Singh, “<a href="https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lecs.html">Data Processing and Information Theory Course Notes</a>” <a href="#fnref:9" class="reversefootnote">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:9:2" class="reversefootnote">&#8617;<sup>3</sup></a> <a href="#fnref:9:3" class="reversefootnote">&#8617;<sup>4</sup></a> <a href="#fnref:9:4" class="reversefootnote">&#8617;<sup>5</sup></a></p>
    </li>
    <li id="fn:11">
      <p>L. Faivishevsky, J. Goldberger, “<a href="https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/faivishevsky.pdf">A Nonparametric Information Theoretic Clustering Algorithm</a>” <a href="#fnref:11" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>A. Krause, A. Singh, C. Guestrin, “<a href="http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf">Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a>” <a href="#fnref:10" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>A. Kraskov, H. Stoegbauer, P. Grassberger, “<a href="https://arxiv.org/abs/cond-mat/0305641">Estimating Mutual Information</a>” <a href="#fnref:5" class="reversefootnote">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:8">
      <p>J. Walters-Williams, Y. Li, “<a href="https://link.springer.com/chapter/10.1007/978-3-642-02962-2_49">Estimating Mutual Information: A Survey</a>” <a href="#fnref:8" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>T. Cover, J. Thomas, “Elements of Information Theory” <a href="#fnref:1" class="reversefootnote">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote">&#8617;<sup>4</sup></a> <a href="#fnref:1:4" class="reversefootnote">&#8617;<sup>5</sup></a> <a href="#fnref:1:5" class="reversefootnote">&#8617;<sup>6</sup></a></p>
    </li>
    <li id="fn:6">
      <p>K. Marton, “<a href="https://www.itsoc.org/resources/videos/isit-2013-istanbul/MartonISIT2013.pdf">Distance Divergence Inequalities</a>” <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>K. Marton, “<a href="https://ieeexplore.ieee.org/document/1057176">A simple proof of the blowing-up lemma</a>” <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>A. Gibbs, F. Su, “<a href="https://arxiv.org/abs/math/0209021">On choosing and bounding probability metrics</a>” <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>T. Naftali, N. Zagaslavsky, “<a href="https://arxiv.org/abs/1503.02406">Deep Learning and the Information Bottleneck Principle</a>” <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>A. Tsybakov, “Introduction to Nonparametric Estimation” <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
