# Information Theory and Statistics Reading Group Meetings

## Winter Quarter

[3/5/19] Tong Yitang presents on Lecture 7 of [^9]. Topics include maximum entropy density estimation, information projection, and the duality of maximum entropy with maximum likelihood estimation.

[2/26/19] David Weber presents on Lecture 6 of [^9]. Topics include graphical models.

[2/19/19] David Weber and Dmitry Shemetov present on Lecture 6 of [^9]. Topics include mutual information estimators and applications of mutual information to machine learning.

[2/12/19] Dmitry Shemetov presents on Lecture 4 and 5 of [^9]. Topics include properties of submodular functions, maximizing submodular funcions, and an information theoretic clustering algorithm [^11].

[01/29/19] Dmitry Shemetov presents on Lecture 3 of [^9]. Topics include the data processing inequality, submodular functions, submodularity of mutual information, maximizing submodular functions, and an application of sensor placement [^10].

[01/22/19] David Weber revisits [^5] and presents on [^8]. Topics include estimating mutual information.

[01/15/19] Cong Xu presents on Chapter 5 "Data Compression" of [^1].

## Fall Quarter

[12/13/2018] Dmitry Shemetov presents an introduction to some work by Katalin Marton [^6]: the blowing up lemma [^7], single-letter characterization, Ornstein's copying lemma, and Talagrand's theorem.

[12/07/2018] David Weber lays the foundations with Chapter 8 "Differential Entropy" of [^1] and then presents on "Estimating Mutual Information" [^5].

[11/29/2018] Yiqun Shao presents on [^4].

[11/09/2018] Stephen Sheng presents on [^3].

[11/02/2018] Group read of Chapter 11 "Information Theory and Statistics" of [^1].

[10/26/2018] Group read of Chapter 6 "Gambling and Data Compression" of [^1].

[10/19/2018] Dmitry Shemetov presents on Chapter 2 "Lower bounds on minimax risk" of [^2].

[10/12/2018] A problem solving session dedicated to Chapter 2 of [^1].

[10/05/2018] Dmitry Shemetov presents the foundational material from Chapter 2 "Entropy, Relative Entropy, Mutual Information" of [^1].

[9/28/2018] Organizing meeting.

## References

[^1]: T. Cover, J. Thomas, "Elements of Information Theory"
[^2]: A. Tsybakov, "Introduction to Nonparametric Estimation"
[^3]: T. Naftali, N. Zagaslavsky, "[Deep Learning and the Information Bottleneck Principle](https://arxiv.org/abs/1503.02406)"
[^4]: A. Gibbs, F. Su, "[On choosing and bounding probability metrics](https://arxiv.org/abs/math/0209021)"
[^5]: A. Kraskov, H. Stoegbauer, P. Grassberger, "[Estimating Mutual Information](https://arxiv.org/abs/cond-mat/0305641)"
[^6]: K. Marton, "[Distance Divergence Inequalities](https://www.itsoc.org/resources/videos/isit-2013-istanbul/MartonISIT2013.pdf)"
[^7]: K. Marton, "[A simple proof of the blowing-up lemma](https://ieeexplore.ieee.org/document/1057176)"
[^8]: J. Walters-Williams, Y. Li, "[Estimating Mutual Information: A Survey](https://link.springer.com/chapter/10.1007/978-3-642-02962-2_49)"
[^9]: A. Singh, "[Data Processing and Information Theory Course Notes](https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/lecs.html)"
[^10]: A. Krause, A. Singh, C. Guestrin, "[Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies](http://www.jmlr.org/papers/volume9/krause08a/krause08a.pdf)"
[^11]: L. Faivishevsky, J. Goldberger, "[A Nonparametric Information Theoretic Clustering Algorithm](https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/faivishevsky.pdf)"
