%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Dmitry Shemetov at 2018-07-13 23:43:32 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{zhu_quantized_2015,
	Abstract = {We formulate the notion of minimax estimation under storage or communication constraints, and prove an extension to Pinsker's theorem for nonparametric estimation over Sobolev ellipsoids. Placing limits on the number of bits used to encode any estimator, we give tight lower and upper bounds on the excess risk due to quantization in terms of the number of bits, the signal size, and the noise level. This establishes the Pareto optimal tradeoff between storage and risk under quantization constraints for Sobolev spaces. Our results and proof techniques combine elements of rate distortion theory and minimax analysis. The proposed quantized estimation scheme, which shows achievability of the lower bounds, is adaptive in the usual statistical sense, achieving the optimal quantized minimax rate without knowledge of the smoothness parameter of the Sobolev space. It is also adaptive in a computational sense, as it constructs the code only after observing the data, to dynamically allocate more codewords to blocks where the estimated signal size is large. Simulations are included that illustrate the effect of quantization on statistical risk.},
	Author = {Zhu, Yuancheng and Lafferty, John},
	Date-Added = {2018-07-14 06:43:13 +0000},
	Date-Modified = {2018-07-14 06:43:13 +0000},
	Journal = {arXiv:1503.07368 [math, stat]},
	Keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	Month = mar,
	Note = {arXiv: 1503.07368},
	Title = {Quantized {Nonparametric} {Estimation} over {Sobolev} {Ellipsoids}},
	Url = {http://arxiv.org/abs/1503.07368},
	Urldate = {2018-07-14},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.07368}}

@article{russo_how_2015,
	Abstract = {Modern data is messy and high-dimensional, and it is often not clear a priori what are the right questions to ask. Instead, the analyst typically needs to use the data to search for interesting analyses to perform and hypotheses to test. This is an adaptive process, where the choice of analysis to be performed next depends on the results of the previous analyses on the same data. Ultimately, which results are reported can be heavily influenced by the data. It is widely recognized that this process, even if well-intentioned, can lead to biases and false discoveries, contributing to the crisis of reproducibility in science. But while \%the adaptive nature of exploration any data-exploration renders standard statistical theory invalid, experience suggests that different types of exploratory analysis can lead to disparate levels of bias, and the degree of bias also depends on the particulars of the data set. In this paper, we propose a general information usage framework to quantify and provably bound the bias and other error metrics of an arbitrary exploratory analysis. We prove that our mutual information based bound is tight in natural settings, and then use it to give rigorous insights into when commonly used procedures do or do not lead to substantially biased estimation. Through the lens of information usage, we analyze the bias of specific exploration procedures such as filtering, rank selection and clustering. Our general framework also naturally motivates randomization techniques that provably reduces exploration bias while preserving the utility of the data analysis. We discuss the connections between our approach and related ideas from differential privacy and blinded data analysis, and supplement our results with illustrative simulations.},
	Author = {Russo, Daniel and Zou, James},
	Date-Added = {2018-07-14 01:09:04 +0000},
	Date-Modified = {2018-07-14 01:09:04 +0000},
	File = {arXiv\:1511.05219 PDF:/Users/dmitron/Zotero/storage/KW9BJ6HV/Russo and Zou - 2015 - How much does your data exploration overfit Contr.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/WQ3PHUIP/1511.html:text/html},
	Journal = {arXiv:1511.05219 [cs, stat]},
	Keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = nov,
	Note = {arXiv: 1511.05219},
	Shorttitle = {How much does your data exploration overfit?},
	Title = {How much does your data exploration overfit? {Controlling} bias via information usage},
	Url = {http://arxiv.org/abs/1511.05219},
	Urldate = {2018-07-14},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1511.05219}}

@article{xu_information-theoretic2_2017,
	Abstract = {We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.},
	Annote = {Comment: Final version, accepted to NIPS 2017},
	Author = {Xu, Aolin and Raginsky, Maxim},
	Date-Added = {2018-07-14 01:09:04 +0000},
	Date-Modified = {2018-07-14 01:09:21 +0000},
	File = {arXiv\:1705.07809 PDF:/Users/dmitron/Zotero/storage/Z3PCKJL7/Xu and Raginsky - 2017 - Information-theoretic analysis of generalization c.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/WXFSG9IK/1705.html:text/html},
	Journal = {arXiv:1705.07809 [cs, math, stat]},
	Keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = may,
	Note = {arXiv: 1705.07809},
	Title = {Information-theoretic analysis of generalization capability of learning algorithms},
	Url = {http://arxiv.org/abs/1705.07809},
	Urldate = {2018-07-14},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.07809}}

@article{steinhardt_memory_nodate,
	Abstract = {If a concept class can be represented with a certain amount of memory, can it be efficiently learned with the same amount of memory? What concepts can be efficiently learned by algorithms that extract only a few bits of information from each example? We introduce a formal framework for studying these questions, and investigate the relationship between the fundamental resources of memory or communication and the sample complexity of the learning task. We relate our memorybounded and communication-bounded learning models to the well-studied statistical query model. This connection can be leveraged to obtain both upper and lower bounds: we show strong lower bounds on learning parity functions with bounded communication, as well as upper bounds on solving sparse linear regression problems with limited memory.},
	Author = {Steinhardt, Jacob},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {Steinhardt - Memory, Communication, and Statistical Queries.pdf:/Users/dmitron/Zotero/storage/MTNK83FC/Steinhardt - Memory, Communication, and Statistical Queries.pdf:application/pdf;Steinhardt - Memory, Communication, and Statistical Queries.pdf:/Users/dmitron/Zotero/storage/I6NKU4JS/Steinhardt - Memory, Communication, and Statistical Queries.pdf:application/pdf},
	Language = {en},
	Pages = {27},
	Title = {Memory, {Communication}, and {Statistical} {Queries}}}

@inproceedings{braverman_communication_2016,
	Abstract = {We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the m machines receives n data points from a d-dimensional Gaussian distribution with unknown mean θ which is promised to be k-sparse. The machines communicate by message passing and aim to estimate the mean θ. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed sparse linear regression problem: to achieve the statistical minimax error, the total communication is at least Ω(min\{n, d\}m), where n is the number of observations that each machine receives and d is the ambient dimension. These lower bound results improve upon Shamir (NIPS'14) and Steinhardt, Duchi (COLT'15) by allowing a multi-round interactive communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation.},
	Author = {Braverman, Mark and Garg, Ankit and Ma, Tengyu and Nguyen, Huy L. and Woodruff, David P.},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Doi = {10.1145/2897518.2897582},
	File = {Braverman et al. - 2016 - Communication lower bounds for statistical estimat.pdf:/Users/dmitron/Zotero/storage/JC4LVZ53/Braverman et al. - 2016 - Communication lower bounds for statistical estimat.pdf:application/pdf;Braverman et al. - 2016 - Communication lower bounds for statistical estimat.pdf:/Users/dmitron/Zotero/storage/H9QTWJLF/Braverman et al. - 2016 - Communication lower bounds for statistical estimat.pdf:application/pdf},
	Isbn = {978-1-4503-4132-5},
	Language = {en},
	Pages = {1011--1020},
	Publisher = {ACM Press},
	Title = {Communication lower bounds for statistical estimation problems via a distributed data processing inequality},
	Url = {http://dl.acm.org/citation.cfm?doid=2897518.2897582},
	Urldate = {2018-05-03},
	Year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?doid=2897518.2897582},
	Bdsk-Url-2 = {https://doi.org/10.1145/2897518.2897582}}

@inproceedings{xu_converses_2015,
	Abstract = {We consider the problem of distributed estimation, where local processors observe independent samples conditioned on a common random parameter of interest, map the observations to a finite number of bits, and send these bits to a remote estimator over independent noisy channels. We derive converse results for this problem, such as lower bounds on Bayes risk. The main technical tools include a lower bound on the Bayes risk via mutual information and small ball probability, as well as strong data processing inequalities for the relative entropy. Our results can recover and improve some existing results on distributed estimation with noiseless channels, and also capture the effect of noisy channels on the estimation performance.},
	Author = {Xu, Aolin and Raginsky, Maxim},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Doi = {10.1109/ISIT.2015.7282881},
	File = {Xu and Raginsky - 2015 - Converses for distributed estimation via strong da.pdf:/Users/dmitron/Zotero/storage/DKGD5YXN/Xu and Raginsky - 2015 - Converses for distributed estimation via strong da.pdf:application/pdf},
	Isbn = {978-1-4673-7704-1},
	Language = {en},
	Month = jun,
	Pages = {2376--2380},
	Publisher = {IEEE},
	Title = {Converses for distributed estimation via strong data processing inequalities},
	Url = {http://ieeexplore.ieee.org/document/7282881/},
	Urldate = {2018-05-03},
	Year = {2015},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7282881/},
	Bdsk-Url-2 = {https://doi.org/10.1109/ISIT.2015.7282881}}

@article{duchi_optimality_2014,
	Abstract = {Large data sets often require performing distributed statistical estimation, with a full data set split across multiple machines and limited communication between machines. To study such scenarios, we define and study some refinements of the classical minimax risk that apply to distributed settings, comparing to the performance of estimators with access to the entire data. Lower bounds on these quantities provide a precise characterization of the minimum amount of communication required to achieve the centralized minimax risk. We study two classes of distributed protocols: one in which machines send messages independently over channels without feedback, and a second allowing for interactive communication, in which a central server broadcasts the messages from a given machine to all other machines. We establish lower bounds for a variety of problems, including location estimation in several families and parameter estimation in different types of regression models. Our results include a novel class of quantitative data-processing inequalities used to characterize the effects of limited communication.},
	Annote = {Comment: 34 pages, 1 figure. Preliminary version appearing in Neural Information Processing Systems 2013 (http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints)},
	Author = {Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J. and Zhang, Yuchen},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {Duchi et al. - 2014 - Optimality guarantees for distributed statistical .pdf:/Users/dmitron/Zotero/storage/K2UDT5PT/Duchi et al. - 2014 - Optimality guarantees for distributed statistical .pdf:application/pdf;Duchi et al. - 2014 - Optimality guarantees for distributed statistical .pdf:/Users/dmitron/Zotero/storage/D4S8R5WV/Duchi et al. - 2014 - Optimality guarantees for distributed statistical .pdf:application/pdf},
	Journal = {arXiv:1405.0782 [cs, math, stat]},
	Keywords = {Computer Science - Information Theory, Computer Science - Learning, Mathematics - Statistics Theory},
	Language = {en},
	Month = may,
	Note = {arXiv: 1405.0782},
	Title = {Optimality guarantees for distributed statistical estimation},
	Url = {http://arxiv.org/abs/1405.0782},
	Urldate = {2018-05-03},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1405.0782}}

@article{duchi_distance-based_2013,
	Abstract = {In this technical note, we give two extensions of the classical Fano inequality in information theory. The first extends Fano's inequality to the setting of estimation, providing lower bounds on the probability that an estimator of a discrete quantity is within some distance t of the quantity. The second inequality extends our bound to a continuum setting and provides a volume-based bound. We illustrate how these inequalities lead to direct and simple proofs of several statistical minimax lower bounds.},
	Annote = {Comment: 16 pages, 1 figure},
	Author = {Duchi, John C. and Wainwright, Martin J.},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {Duchi and Wainwright - 2013 - Distance-based and continuum Fano inequalities wit.pdf:/Users/dmitron/Zotero/storage/442PQF7S/Duchi and Wainwright - 2013 - Distance-based and continuum Fano inequalities wit.pdf:application/pdf;Duchi and Wainwright - 2013 - Distance-based and continuum Fano inequalities wit.pdf:/Users/dmitron/Zotero/storage/VEUQ4M2H/Duchi and Wainwright - 2013 - Distance-based and continuum Fano inequalities wit.pdf:application/pdf},
	Journal = {arXiv:1311.2669 [cs, math, stat]},
	Keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
	Language = {en},
	Month = nov,
	Note = {arXiv: 1311.2669},
	Title = {Distance-based and continuum {Fano} inequalities with applications to statistical estimation},
	Url = {http://arxiv.org/abs/1311.2669},
	Urldate = {2018-05-03},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1311.2669}}

@article{shamir_fundamental_2013,
	Abstract = {Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings.},
	Annote = {Comment: Full version of NIPS 2014 paper},
	Author = {Shamir, Ohad},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {Shamir - 2013 - Fundamental Limits of Online and Distributed Algor.pdf:/Users/dmitron/Zotero/storage/4I2NTNYN/Shamir - 2013 - Fundamental Limits of Online and Distributed Algor.pdf:application/pdf},
	Journal = {arXiv:1311.3494 [cs, stat]},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Language = {en},
	Month = nov,
	Note = {arXiv: 1311.3494},
	Title = {Fundamental {Limits} of {Online} and {Distributed} {Algorithms} for {Statistical} {Learning} and {Estimation}},
	Url = {http://arxiv.org/abs/1311.3494},
	Urldate = {2018-05-03},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1311.3494}}

@phdthesis{xu_information-theoretic_2016,
	Author = {Xu, Aolin},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {Xu.InformationTheoreticLimitationsDistributed.pdf:/Users/dmitron/Zotero/storage/3DHQ9PI7/Xu.InformationTheoreticLimitationsDistributed.pdf:application/pdf},
	School = {Diss. University of Illinois at Urbana-Champaign},
	Title = {Information-theoretic limitations of distributed information processing.},
	Year = {2016}}

@article{yang_information-theoretic_1999,
	Abstract = {We present some general results determining minimax bounds on statistical risk for density estimation based on certain information-theoretic considerations. These bounds depend only on metric entropy conditions and are used to identify the minimax rates of convergence.},
	Author = {Yang, Yuhong and Barron, Andrew},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Doi = {10.1214/aos/1017939142},
	File = {Full Text PDF:/Users/dmitron/Zotero/storage/JQ5WSXNY/Yang and Barron - 1999 - Information-theoretic determination of minimax rat.pdf:application/pdf;Snapshot:/Users/dmitron/Zotero/storage/FPM9I6HE/1017939142.html:text/html},
	Issn = {0090-5364, 2168-8966},
	Journal = {The Annals of Statistics},
	Keywords = {density estimation, Kullback-Leibler distance, metric entropy, Minimax risk},
	Language = {en},
	Month = oct,
	Mrnumber = {MR2001g:62006},
	Number = {5},
	Pages = {1564--1599},
	Title = {Information-theoretic determination of minimax rates of convergence},
	Url = {https://projecteuclid.org/euclid.aos/1017939142},
	Urldate = {2018-05-12},
	Volume = {27},
	Year = {1999},
	Zmnumber = {0978.62008},
	Bdsk-Url-1 = {https://projecteuclid.org/euclid.aos/1017939142},
	Bdsk-Url-2 = {https://doi.org/10.1214/aos/1017939142}}

@article{raginsky_strong_2014,
	Abstract = {The noisiness of a channel can be measured by comparing suitable functionals of the input and output distributions. For instance, the worst-case ratio of output relative entropy to input relative entropy for all possible pairs of input distributions is bounded from above by unity, by the data processing theorem. However, for a fixed reference input distribution, this quantity may be strictly smaller than one, giving so-called strong data processing inequalities (SDPIs). The same considerations apply to an arbitrary \${\textbackslash}Phi\$-divergence. This paper presents a systematic study of optimal constants in SDPIs for discrete channels, including their variational characterizations, upper and lower bounds, structural results for channels on product probability spaces, and the relationship between SDPIs and so-called \${\textbackslash}Phi\$-Sobolev inequalities (another class of inequalities that can be used to quantify the noisiness of a channel by controlling entropy-like functionals of the input distribution by suitable measures of input-output correlation). Several applications to information theory, discrete probability, and statistical physics are discussed.},
	Annote = {Comment: 74 pages, 3 figures; accepted to IEEE Transactions on Information Theory},
	Author = {Raginsky, Maxim},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	File = {arXiv\:1411.3575 PDF:/Users/dmitron/Zotero/storage/G8TE3JWX/Raginsky - 2014 - Strong data processing inequalities and \$Phi\$-Sob.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/7EZK82QQ/1411.html:text/html},
	Journal = {arXiv:1411.3575 [cs, math]},
	Keywords = {Computer Science - Information Theory, Mathematics - Probability},
	Month = nov,
	Note = {arXiv: 1411.3575},
	Title = {Strong data processing inequalities and \${\textbackslash}{Phi}\$-{Sobolev} inequalities for discrete channels},
	Url = {http://arxiv.org/abs/1411.3575},
	Urldate = {2018-05-22},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1411.3575}}

@article{jordan_communication-efficient_2018,
	Abstract = {We present a Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical inference problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon naive averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax-optimal estimator with controlled communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. We present both theoretical analysis and experiments to explore the properties of the CSL approximation.},
	Author = {Jordan, Michael I. and Lee, Jason D. and Yang, Yun},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Doi = {10.1080/01621459.2018.1429274},
	File = {Full Text PDF:/Users/dmitron/Zotero/storage/AF6YMREN/Jordan et al. - 2018 - Communication-Efficient Distributed Statistical In.pdf:application/pdf;Snapshot:/Users/dmitron/Zotero/storage/7XCAMPYC/01621459.2018.html:text/html},
	Issn = {0162-1459},
	Journal = {Journal of the American Statistical Association},
	Keywords = {communication efficiency, Distributed inference, likelihood approximation},
	Month = feb,
	Number = {ja},
	Pages = {0--0},
	Title = {Communication-{Efficient} {Distributed} {Statistical} {Inference}},
	Url = {https://doi.org/10.1080/01621459.2018.1429274},
	Urldate = {2018-07-06},
	Volume = {0},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2018.1429274}}

@incollection{zhang_information-theoretic_2013,
	Author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I and Wainwright, Martin J},
	Booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	File = {NIPS Snapshort:/Users/dmitron/Zotero/storage/BLGH8BFN/4902-information-theoretic-lower-bounds-for-distribu.html:text/html;Zhang et al. - 2013 - Information-theoretic lower bounds for distributed.pdf:/Users/dmitron/Zotero/storage/KDISI6MS/Zhang et al. - 2013 - Information-theoretic lower bounds for distributed.pdf:application/pdf},
	Pages = {2328--2336},
	Publisher = {Curran Associates, Inc.},
	Title = {Information-theoretic lower bounds for distributed statistical estimation with communication constraints},
	Url = {http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints.pdf},
	Urldate = {2018-07-06},
	Year = {2013},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4902-information-theoretic-lower-bounds-for-distributed-statistical-estimation-with-communication-constraints.pdf}}

@article{xu_information-theoretic_2017,
	Abstract = {We derive lower bounds on the Bayes risk in decentralized estimation, where the estimator does not have direct access to the random samples generated conditionally on the random parameter of interest, but only to the data received from local processors that observe the samples. The received data are subject to communication constraints due to the quantization and the noisy communication channels from the processors to the estimator. We first derive general lower bounds on the Bayes risk using information-theoretic quantities, such as mutual information, information density, small ball probability, and differential entropy. We then apply these lower bounds to the decentralized case, using strong data processing inequalities to quantify the contraction of information due to communication constraints. We treat the cases of a single processor and of multiple processors, where the samples observed by different processors may be conditionally dependent given the parameter, for noninteractive and interactive communication protocols. Our results recover and improve recent lower bounds on the Bayes risk and the minimax risk for certain decentralized estimation problems, where previously only conditionally independent sample sets and noiseless channels have been considered. Moreover, our results provide a general way to quantify the degradation of estimation performance caused by distributing resources to multiple processors, which is only discussed for specific examples in existing works.},
	Author = {Xu, A. and Raginsky, M.},
	Date-Added = {2018-07-06 20:49:08 +0000},
	Date-Modified = {2018-07-06 20:49:08 +0000},
	Doi = {10.1109/TIT.2016.2646342},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/DEG9QAI8/7801953.html:text/html;IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/NDSF6IIV/Xu and Raginsky - 2017 - Information-Theoretic Lower Bounds on Bayes Risk i.pdf:application/pdf},
	Issn = {0018-9448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Bayes methods, Bayes risk, Communication channels, communication constraints, conditionally independent sample sets, Data processing, decentralized estimation, decentralized estimation problems, Distortion, Entropy, Estimation, estimation theory, information theory, information-theoretic lower bounds, interactive communication protocols, minimax risk, multiple processors, Mutual information, Neyman-Pearson converse, noisy communication channels, noninteractive communication protocols, Program processors, protocols, quantisation (signal), quantization, small ball probability, strong data processing inequalities, strong data processing inequality, telecommunication channels},
	Month = mar,
	Number = {3},
	Pages = {1580--1600},
	Title = {Information-{Theoretic} {Lower} {Bounds} on {Bayes} {Risk} in {Decentralized} {Estimation}},
	Volume = {63},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2016.2646342}}

@book{billingsley2008probability,
	Author = {Billingsley, Patrick},
	Date-Added = {2018-06-01 19:58:01 +0000},
	Date-Modified = {2018-06-01 19:58:01 +0000},
	Publisher = {John Wiley \& Sons},
	Title = {Probability and measure},
	Year = {2008}}

@article{heffernan_unbiased_1997,
	Abstract = {We obtain an estimator of the rth central moment of a distribution, which is unbiased for all distributions for which the first r moments exist. We do this by finding the kernel which allows the rth central moment to be written as a regular statistical functional. The U-statistic associated with this kernel is the unique symmetric unbiased estimator of the rth central moment, and, for each distribution, it has minimum variance among all estimators which are unbiased for all these distributions.},
	Author = {Heffernan, Peter M.},
	Date-Added = {2018-06-01 18:28:26 +0000},
	Date-Modified = {2018-06-01 18:28:26 +0000},
	Issn = {0035-9246},
	Journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	Number = {4},
	Pages = {861--863},
	Title = {Unbiased {Estimation} of {Central} {Moments} by {Using} {U}-{Statistics}},
	Url = {http://www.jstor.org/stable/2985201},
	Urldate = {2018-06-01},
	Volume = {59},
	Year = {1997},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2985201}}

@article{ruiz_espejo_optimal_2013,
	Abstract = {In this paper, we have treated the problem of estimating some population central moments under distribution-free setting. Uniformly minimum variance unbiased estimators for some population central moments have been derived. Some examples of unbiased estimators of central moments have been given under various sampling designs such as simple random sampling with replacement (srsr) or without replacement (srs), probability proportional to size with replacement (ppsr) and probability graduated variable proportional to size without replacement (pgvps). An optimal unbiased estimator of the third population central moment is proposed and extended to some real situations. Some optimal unbiased estimators of the fourth population central moment are given. Several optimal unbiased estimators of the variance of the ``sample quasivariance estimator'' are identified. Finally, computer programs in R implementing all of the estimators are given.},
	Author = {Ruiz Espejo, Mariano and Delgado Pineda, Miguel and Nadarajah, Saralees},
	Date-Added = {2018-06-01 18:27:54 +0000},
	Date-Modified = {2018-06-01 18:27:54 +0000},
	Doi = {10.1007/s40300-013-0006-z},
	File = {Ruiz Espejo et al. - 2013 - Optimal unbiased estimation of some population cen.pdf:/Users/dmitron/Zotero/storage/4I3ZM2C9/Ruiz Espejo et al. - 2013 - Optimal unbiased estimation of some population cen.pdf:application/pdf},
	Issn = {0026-1424, 2281-695X},
	Journal = {METRON},
	Language = {en},
	Month = jun,
	Number = {1},
	Pages = {39--62},
	Title = {Optimal unbiased estimation of some population central moments},
	Url = {http://link.springer.com/10.1007/s40300-013-0006-z},
	Urldate = {2018-06-01},
	Volume = {71},
	Year = {2013},
	Bdsk-Url-1 = {http://link.springer.com/10.1007/s40300-013-0006-z},
	Bdsk-Url-2 = {https://doi.org/10.1007/s40300-013-0006-z}}

@article{tyagi_systematic_2013,
	Abstract = {In recent years, Wireless Sensor Networks (WSNs) have emerged as a new powerful technology used in many applications such as military operations, surveillance system, Intelligent Transport Systems (ITS) etc. These networks consist of many Sensor Nodes (SNs), which are not only used for monitoring but also capturing the required data from the environment. Most of the research proposals on WSNs have been developed keeping in view of minimization of energy during the process of extracting the essential data from the environment where SNs are deployed. The primary reason for this is the fact that the SNs are operated on battery which discharges quickly after each operation. It has been found in literature that clustering is the most common technique used for energy aware routing in WSNs. The most popular protocol for clustering in WSNs is Low Energy Adaptive Clustering Hierarchy (LEACH) which is based on adaptive clustering technique. This paper provides the taxonomy of various clustering and routing techniques in WSNs based upon metrics such as power management, energy management, network lifetime, optimal cluster head selection, multihop data transmission etc. A comprehensive discussion is provided in the text highlighting the relative advantages and disadvantages of many of the prominent proposals in this category which helps the designers to select a particular proposal based upon its merits over the others.},
	Author = {Tyagi, Sudhanshu and Kumar, Neeraj},
	Date-Added = {2018-05-30 22:54:44 +0000},
	Date-Modified = {2018-05-30 22:54:44 +0000},
	Doi = {10.1016/j.jnca.2012.12.001},
	Issn = {1084-8045},
	Journal = {Journal of Network and Computer Applications},
	Keywords = {Routing, WSN, Clustering, Energy management, Sensor node},
	Month = mar,
	Number = {2},
	Pages = {623--645},
	Title = {A systematic review on clustering and routing techniques based upon {LEACH} protocol for wireless sensor networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S1084804512002482},
	Urldate = {2018-05-30},
	Volume = {36},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1084804512002482},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jnca.2012.12.001}}

@inproceedings{dasgupta_efficient_2003,
	Abstract = {The rapid advances in processor, memory, and radio technology have enabled the development of distributed networks of small, inexpensive nodes that are capable of sensing, computation, and wireless communication. Sensor networks of the future are envisioned to revolutionize the paradigm of collecting and processing information in diverse environments. However, the severe energy constraints and limited computing resources of the sensors, present major challenges for such a vision to become a reality. We consider a network of energy-constrained sensors that are deployed over a region. Each sensor periodically produces information as it monitors its vicinity. The basic operation in such a network is the systematic gathering and transmission of sensed data gathering and transmission of sensed data to a base station for further processing. During data gathering, sensors have the ability to perform in-network aggregation (fusion) of data packets enroute to the base station. The lifetime of such a sensor system is the time during which we can gather information from all the sensors to the base station. A key challenge in data gathering is to maximize the system lifetime, given the energy constraints of the sensors. Given the location of sensors and the base station and the available energy at each sensor, we are interested in finding an efficient manner in which data should be collected from all the sensors and transmitted to the base station, such that the system lifetime is maximized. This is the maximum lifetime data-gathering problem. In this paper, we describe a heuristic to solve the data-gathering problem with aggregation in sensor networks. Our experimental results demonstrate that the proposed algorithm significantly outperform previous methods, in terms of system lifetime.},
	Author = {Dasgupta, K. and Kalpakis, K. and Namjoshi, P.},
	Booktitle = {2003 {IEEE} {Wireless} {Communications} and {Networking}, 2003. {WCNC} 2003.},
	Date-Added = {2018-05-30 22:47:32 +0000},
	Date-Modified = {2018-05-30 22:47:32 +0000},
	Doi = {10.1109/WCNC.2003.1200685},
	File = {IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/JCSLELAU/Dasgupta et al. - 2003 - An efficient clustering-based heuristic for data g.pdf:application/pdf},
	Keywords = {Sensor fusion, Sensor systems, Computer science, wireless sensor networks, Wireless sensor networks, data fusion, sensor fusion, Acoustic sensors, Base stations, Computer networks, data acquisition, data aggregation, data gathering, data packet, Distributed computing, efficient clustering-based heuristic, energy constraint, energy-constrained sensor, in-network aggregation, Intelligent networks, packet radio networks, pattern clustering, radio technology, sensor network, Sensor phenomena and characterization, system lifetime, wireless communication},
	Month = mar,
	Pages = {1948--1953 vol.3},
	Title = {An efficient clustering-based heuristic for data gathering and aggregation in sensor networks},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/WCNC.2003.1200685}}

@inproceedings{heinzelman_energy-efficient_2000,
	Abstract = {Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multi-hop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster based station (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show the LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional outing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated.},
	Author = {Heinzelman, W. R. and Chandrakasan, A. and Balakrishnan, H.},
	Booktitle = {Proceedings of the 33rd {Annual} {Hawaii} {International} {Conference} on {System} {Sciences}},
	Date-Added = {2018-05-30 22:43:06 +0000},
	Date-Modified = {2018-05-30 22:43:06 +0000},
	Doi = {10.1109/HICSS.2000.926982},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/VUDPJWL7/926982.html:text/html;IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/5JM3QGZY/Heinzelman et al. - 2000 - Energy-efficient communication protocol for wirele.pdf:application/pdf},
	Keywords = {civil applications, clustering-based protocol, data fusion, dynamic networks, energy dissipation, Energy dissipation, Energy efficiency, energy load distribution, energy-efficient communication protocol, LEACH, localized coordination, Low-Energy Adaptive Clustering Hierarchy, microsensors, Microsensors, military applications, military communication, mobile radio, Monitoring, protocols, randomized local cluster based station rotation, reliable monitoring, robustness, routing protocol, Routing protocols, scalability, Scalability, sensor fusion, simulations, Spread spectrum communication, Telecommunication network reliability, telecommunication network routing, useful system lifetime, Wireless application protocol, Wireless communication, wireless distributed microsensor system},
	Month = jan,
	Pages = {10 pp. vol.2--},
	Title = {Energy-efficient communication protocol for wireless microsensor networks},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1109/HICSS.2000.926982}}

@inproceedings{li_scaling_2014,
	Abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. To demonstrate the scalability of the proposed framework, we show experimental results on petabytes of real data with billions of examples and parameters on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
	Address = {Berkeley, CA, USA},
	Author = {Li, Mu and Andersen, David G. and Park, Jun Woo and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
	Booktitle = {Proceedings of the 11th {USENIX} {Conference} on {Operating} {Systems} {Design} and {Implementation}},
	Date-Added = {2018-05-30 20:47:40 +0000},
	Date-Modified = {2018-05-30 20:47:40 +0000},
	File = {Li - 2014 - Scaling Distributed Machine Learning with the Para.pdf:/Users/dmitron/Zotero/storage/7Z3Z6N9T/Li - 2014 - Scaling Distributed Machine Learning with the Para.pdf:application/pdf},
	Isbn = {978-1-931971-16-4},
	Pages = {583--598},
	Publisher = {USENIX Association},
	Series = {{OSDI}'14},
	Title = {Scaling {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	Url = {http://dl.acm.org/citation.cfm?id=2685048.2685095},
	Urldate = {2018-05-30},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2685048.2685095}}

@article{al-jarrah_efficient_2015,
	Abstract = {With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years -- in fact, as much as 90\% of current data were created in the last couple of years -- a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven -- the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas' structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.},
	Author = {Al-Jarrah, Omar Y. and Yoo, Paul D. and Muhaidat, Sami and Karagiannidis, George K. and Taha, Kamal},
	Date-Added = {2018-05-30 18:23:33 +0000},
	Date-Modified = {2018-05-30 18:23:33 +0000},
	Doi = {10.1016/j.bdr.2015.04.001},
	File = {ScienceDirect Full Text PDF:/Users/dmitron/Zotero/storage/FC98UCYQ/Al-Jarrah et al. - 2015 - Efficient Machine Learning for Big Data A Review.pdf:application/pdf;ScienceDirect Snapshot:/Users/dmitron/Zotero/storage/JR6KXEU2/S2214579615000271.html:text/html},
	Issn = {2214-5796},
	Journal = {Big Data Research},
	Keywords = {Big data, Computational modeling, Efficient machine learning, Green computing},
	Month = sep,
	Number = {3},
	Pages = {87--93},
	Series = {Big {Data}, {Analytics}, and {High}-{Performance} {Computing}},
	Shorttitle = {Efficient {Machine} {Learning} for {Big} {Data}},
	Title = {Efficient {Machine} {Learning} for {Big} {Data}: {A} {Review}},
	Url = {http://www.sciencedirect.com/science/article/pii/S2214579615000271},
	Urldate = {2018-05-30},
	Volume = {2},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2214579615000271},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.bdr.2015.04.001}}

@article{jordan_machine_2015,
	Abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	Author = {Jordan, M. I. and Mitchell, T. M.},
	Copyright = {Copyright {\copyright} 2015, American Association for the Advancement of Science},
	Date-Added = {2018-05-30 18:22:54 +0000},
	Date-Modified = {2018-05-30 18:22:54 +0000},
	Doi = {10.1126/science.aaa8415},
	File = {Full Text PDF:/Users/dmitron/Zotero/storage/75QBM5TR/Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf;Snapshot:/Users/dmitron/Zotero/storage/VDLGFXGZ/255.html:text/html},
	Issn = {0036-8075, 1095-9203},
	Journal = {Science},
	Language = {en},
	Month = jul,
	Number = {6245},
	Pages = {255--260},
	Pmid = {26185243},
	Shorttitle = {Machine learning},
	Title = {Machine learning: {Trends}, perspectives, and prospects},
	Url = {http://science.sciencemag.org/content/349/6245/255},
	Urldate = {2018-05-30},
	Volume = {349},
	Year = {2015},
	Bdsk-Url-1 = {http://science.sciencemag.org/content/349/6245/255},
	Bdsk-Url-2 = {https://doi.org/10.1126/science.aaa8415}}

@inproceedings{akbari_energy_2014,
	Abstract = {With the widespread use of wireless sensors, the management of their energy resources has become a topic of research. Wireless sensors usually use batteries as their power supply but in some applications battery replacement can be cumbersome and require considerable amount of time which can affect the process being monitored. It is possible to harvest energy from the sources in nature for wireless sensors. In this article, a review of current alternative energy sources has been demonstrated in order to address the feasibility of their integration with wireless sensor networks.},
	Author = {Akbari, S.},
	Booktitle = {2014 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems}},
	Date-Added = {2018-05-30 18:21:03 +0000},
	Date-Modified = {2018-05-30 18:21:03 +0000},
	Doi = {10.15439/2014F85},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/37FHC4B5/6933124.html:text/html;IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/AVEIHI4Y/Akbari - 2014 - Energy harvesting for wireless sensor networks rev.pdf:application/pdf},
	Keywords = {Batteries, battery replacement, Blades, energy harvesting, Energy harvesting, energy resources, Monitoring, power supply, power supply quality, Sensors, telecommunication power management, Wireless communication, wireless sensor networks, Wireless sensor networks},
	Month = sep,
	Pages = {987--992},
	Title = {Energy harvesting for wireless sensor networks review},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.15439/2014F85}}

@article{aqeel-ur-rehman_review_2014,
	Abstract = {Due to advancement in technologies and reduction in size, sensors are becoming involved in almost every field of life. Agriculture is one of such domains where sensors and their networks are successfully used to get numerous benefits. Selection of sensors and their effective utilization to solve agricultural domain problems has been an arduous task for novice users due to unavailability of conglomerated information in literature. The aim of this paper is to review the need of wireless sensors in Agriculture, WSN technology and their applications in different aspects of agriculture and to report existing system frameworks in agriculture domain.},
	Author = {{Aqeel-ur-Rehman} and Abbasi, Abu Zafar and Islam, Noman and Shaikh, Zubair Ahmed},
	Date-Added = {2018-05-30 18:20:21 +0000},
	Date-Modified = {2018-05-30 18:20:21 +0000},
	Doi = {10.1016/j.csi.2011.03.004},
	File = {ScienceDirect Full Text PDF:/Users/dmitron/Zotero/storage/5XJHPAHQ/Aqeel-ur-Rehman et al. - 2014 - A review of wireless sensors and networks' applica.pdf:application/pdf;ScienceDirect Snapshot:/Users/dmitron/Zotero/storage/CDAKNLIL/S0920548911000353.html:text/html},
	Issn = {0920-5489},
	Journal = {Computer Standards \& Interfaces},
	Keywords = {Agriculture, Framework, Sensor and Actuator Network, Sensors},
	Month = feb,
	Number = {2},
	Pages = {263--270},
	Title = {A review of wireless sensors and networks' applications in agriculture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548911000353},
	Urldate = {2018-05-30},
	Volume = {36},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0920548911000353},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.csi.2011.03.004}}

@inproceedings{zhang_review_2012,
	Abstract = {In recent ten years, wireless sensor network technology has a rapid development. After a brief introduction of the wireless sensor network, some main research results of energy conservation and node deployment is provided. Then the applications of WSN in the medical health, environment and agriculture, intelligent home furnishing and building, military, space and marine exploration are outlined. In addition, we analyze the advantage of WSN in these areas. Finally, we summarize the main factors that affect the applications of wireless sensor network.},
	Author = {Zhang, S. and Zhang, H.},
	Booktitle = {2012 {IEEE} {International} {Conference} on {Automation} and {Logistics}},
	Date-Added = {2018-05-30 18:19:43 +0000},
	Date-Modified = {2018-05-30 18:19:43 +0000},
	Doi = {10.1109/ICAL.2012.6308240},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/T82BYSGB/6308240.html:text/html;IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/NXC2I5YG/Zhang and Zhang - 2012 - A review of wireless sensor networks and its appli.pdf:application/pdf},
	Keywords = {agriculture, Application, building, Educational institutions, energy conservation, Energy conservation, environment, intelligent home furnishing, marine exploration, medical health, military, Monitoring, node deployment, Nodedeploymeng, Protocols, Security, space exploration, Temperature sensors, Wireless sensor network, wireless sensor network technology, wireless sensor networks, Wireless sensor networks, WSN},
	Month = aug,
	Pages = {386--389},
	Title = {A review of wireless sensor networks and its applications},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICAL.2012.6308240}}

@article{liu_atypical_2015,
	Abstract = {Hierarchical routing in wireless sensor networks (WSNs) is a very important topic that has been attracting the research community in the last decade. Typical hierarchical routing is called clustering routing, in which the network is divided into multiple clusters. Recently, some types of atypical hierarchical routing arise, including chain-based, tree-based, grid-based routing, and area-based routing. There are several survey papers that present and compare the hierarchical routing protocols from various perspectives, but a survey on atypical hierarchical routing is still missing. This paper makes a first attempt to provide a comprehensive review on atypical hierarchical routing. We offer a classification of atypical hierarchical routing of WSNs, and give detailed analysis of different logical topologies. The most representative atypical hierarchical routing protocols are described, discussed, and qualitatively compared. In particular, the advantages and disadvantages of different atypical hierarchical routing protocols are analyzed with respect to their significant performances and application scenarios. Finally, we put forward some open issues concerning the design of hierarchical WSNs. This survey aims to provide useful guidance for system designers on how to evaluate and select appropriate logical topologies and hierarchical routing protocols for specific applications.},
	Author = {Liu, X.},
	Date-Added = {2018-05-30 17:28:23 +0000},
	Date-Modified = {2018-05-30 17:28:23 +0000},
	Doi = {10.1109/JSEN.2015.2445796},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/QF7M4VHX/7124398.html:text/html;IEEE Xplore Full Text PDF:/Users/dmitron/Zotero/storage/L9FZDIRJ/Liu - 2015 - Atypical Hierarchical Routing Protocols for Wirele.pdf:application/pdf},
	Issn = {1530-437X},
	Journal = {IEEE Sensors Journal},
	Keywords = {area-based, area-based routing, atypical hierarchical routing, chain-based, chain-based routing, clustering routing, Data communication, Energy consumption, grid-based, grid-based routing, hierarchical routing protocols, logical topologies, multiple clusters, Routing, routing protocols, Routing protocols, Sensors, telecommunication network topology, Topology, tree based routing, tree-based, wireless sensor networks, Wireless sensor networks, WSN},
	Month = oct,
	Number = {10},
	Pages = {5372--5383},
	Shorttitle = {Atypical {Hierarchical} {Routing} {Protocols} for {Wireless} {Sensor} {Networks}},
	Title = {Atypical {Hierarchical} {Routing} {Protocols} for {Wireless} {Sensor} {Networks}: {A} {Review}},
	Volume = {15},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSEN.2015.2445796}}

@book{cover2012elements,
	Author = {Cover, Thomas M and Thomas, Joy A},
	Date-Added = {2018-05-27 02:05:34 +0000},
	Date-Modified = {2018-05-27 02:05:34 +0000},
	Publisher = {John Wiley \& Sons},
	Title = {Elements of information theory},
	Year = {2012}}

@book{bickel2015mathematical,
	Author = {Bickel, Peter J and Doksum, Kjell A},
	Date-Added = {2018-05-27 02:04:48 +0000},
	Date-Modified = {2018-05-27 02:04:48 +0000},
	Publisher = {CRC Press},
	Title = {Mathematical statistics: basic ideas and selected topics, volume I},
	Volume = {117},
	Year = {2015}}

@book{durrett2010probability,
	Author = {Durrett, Rick},
	Date-Added = {2018-05-27 02:04:06 +0000},
	Date-Modified = {2018-05-27 02:04:06 +0000},
	Publisher = {Cambridge university press},
	Title = {Probability: theory and examples},
	Year = {2010}}

@book{tsybakov_introduction_2009,
	Abstract = {Methods of nonparametric estimation are located at the core of modern statistical science. The aim of this book is to give a short but mathematically self-contained introduction to the theory of nonparametric estimation. The emphasis is on the construction of optimal estimators; therefore the concepts of minimax optimality and adaptivity, as well as the oracle approach, occupy the central place in the book. This is a concise text developed from lecture notes and ready to be used for a course on the graduate level. The main idea is to introduce the fundamental concepts of the theory while maintaining the exposition suitable for a first approach in the field. Therefore, the results are not always given in the most general form but rather under assumptions that lead to shorter or more elegant proofs. The book has three chapters. Chapter 1 presents basic nonparametric regression and density estimators and analyzes their properties. Chapter 2 is devoted to a detailed treatment of minimax lower bounds. Chapter 3 develops more advanced topics: Pinsker's theorem, oracle inequalities, Stein shrinkage, and sharp minimax adaptivity.},
	Address = {New York},
	Author = {Tsybakov, Alexandre B.},
	Date-Added = {2018-05-21 22:08:58 +0000},
	Date-Modified = {2018-05-21 22:08:58 +0000},
	File = {Snapshot:/Users/dmitron/Zotero/storage/EU3KKNHP/9780387790510.html:text/html},
	Isbn = {978-0-387-79051-0},
	Language = {en},
	Publisher = {Springer-Verlag},
	Series = {Springer {Series} in {Statistics}},
	Title = {Introduction to {Nonparametric} {Estimation}},
	Url = {//www.springer.com/us/book/9780387790510},
	Urldate = {2018-05-21},
	Year = {2009},
	Bdsk-Url-1 = {//www.springer.com/us/book/9780387790510}}

@article{gibbs_choosing_2002,
	Abstract = {When studying convergence of measures, an important issue is the choice of probability metric. In this review, we provide a summary and some new results concerning bounds among ten important probability metrics/distances that are used by statisticians and probabilists. We focus on these metrics because they are either well-known, commonly used, or admit practical bounding techniques. We summarize these relationships in a handy reference diagram, and also give examples to show how rates of convergence can depend on the metric chosen.},
	Annote = {Comment: To appear, International Statistical Review. Related work at http://www.math.hmc.edu/{\textasciitilde}su/papers.html},
	Author = {Gibbs, Alison L. and Su, Francis Edward},
	Date-Added = {2018-05-13 03:21:36 +0000},
	Date-Modified = {2018-05-13 03:21:36 +0000},
	File = {arXiv\:math/0209021 PDF:/Users/dmitron/Zotero/storage/7NME5LWB/Gibbs and Su - 2002 - On choosing and bounding probability metrics.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/MYHZ9LGK/0209021.html:text/html},
	Journal = {arXiv:math/0209021},
	Keywords = {60B10, Mathematics - Probability},
	Month = sep,
	Note = {arXiv: math/0209021},
	Title = {On choosing and bounding probability metrics},
	Url = {http://arxiv.org/abs/math/0209021},
	Urldate = {2018-05-13},
	Year = {2002},
	Bdsk-Url-1 = {http://arxiv.org/abs/math/0209021}}

@article{guntuboyina_sharp_2013,
	Abstract = {\$f\$-divergences are a general class of divergences between probability measures which include as special cases many commonly used divergences in probability, mathematical statistics and information theory such as Kullback-Leibler divergence, chi-squared divergence, squared Hellinger distance, total variation distance etc. In this paper, we study the problem of maximizing or minimizing an \$f\$-divergence between two probability measures subject to a finite number of constraints on other \$f\$-divergences. We show that these infinite-dimensional optimization problems can all be reduced to optimization problems over small finite dimensional spaces which are tractable. Our results lead to a comprehensive and unified treatment of the problem of obtaining sharp inequalities between \$f\$-divergences. We demonstrate that many of the existing results on inequalities between \$f\$-divergences can be obtained as special cases of our results and we also improve on some existing non-sharp inequalities.},
	Author = {Guntuboyina, Adityanand and Saha, Sujayam and Schiebinger, Geoffrey},
	Date-Added = {2018-05-13 03:14:47 +0000},
	Date-Modified = {2018-05-13 03:14:47 +0000},
	File = {arXiv\:1302.0336 PDF:/Users/dmitron/Zotero/storage/XXFFH557/Guntuboyina et al. - 2013 - Sharp Inequalities for \$f\$-divergences.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/6C2ADRH9/1302.html:text/html},
	Journal = {arXiv:1302.0336 [cs, math, stat]},
	Keywords = {Computer Science - Information Theory, Mathematics - Optimization and Control, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
	Month = feb,
	Note = {arXiv: 1302.0336},
	Title = {Sharp {Inequalities} for \$f\$-divergences},
	Url = {http://arxiv.org/abs/1302.0336},
	Urldate = {2018-05-13},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1302.0336}}

@article{polyanskiy_channel_2010,
	Abstract = {This paper investigates the maximal channel coding rate achievable at a given blocklength and error probability. For general classes of channels new achievability and converse bounds are given, which are tighter than existing bounds for wide ranges of parameters of interest, and lead to tight approximations of the maximal achievable rate for blocklengths n as short as 100. It is also shown analytically that the maximal rate achievable with error probability {\^A}¿ isclosely approximated by C - {\^A}¿(V/n) Q-1({\^A}¿) where C is the capacity, V is a characteristic of the channel referred to as channel dispersion , and Q is the complementary Gaussian cumulative distribution function.},
	Author = {Polyanskiy, Y. and Poor, H. V. and Verdu, S.},
	Date-Added = {2018-05-12 23:25:38 +0000},
	Date-Modified = {2018-05-12 23:25:38 +0000},
	Doi = {10.1109/TIT.2010.2043769},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/I5GJPPFH/5452208.html:text/html},
	Issn = {0018-9448},
	Journal = {IEEE Transactions on Information Theory},
	Keywords = {Achievability, Acoustic noise, AWGN, Capacity planning, channel capacity, Channel capacity, channel coding, Channel coding, channel dispersion, Codes, coding for noisy channels, complementary Gaussian cumulative distribution function, converse, Distribution functions, error probability, Error probability, error statistics, finite blocklength regime, Helium, maximal channel coding rate, Shannon theory, Upper bound},
	Month = may,
	Number = {5},
	Pages = {2307--2359},
	Title = {Channel {Coding} {Rate} in the {Finite} {Blocklength} {Regime}},
	Volume = {56},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIT.2010.2043769}}

@book{polyanskiy2010channel,
	Author = {Polyanskiy, Yury},
	Date-Added = {2018-05-12 23:25:08 +0000},
	Date-Modified = {2018-05-12 23:25:08 +0000},
	Publisher = {Princeton University},
	Title = {Channel coding: non-asymptotic fundamental limits},
	Year = {2010}}

@article{lian_can_2017,
	Abstract = {Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.},
	Author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
	Date-Added = {2018-05-12 23:16:04 +0000},
	Date-Modified = {2018-05-12 23:16:04 +0000},
	File = {arXiv.org Snapshot:/Users/dmitron/Zotero/storage/IYX8RSMZ/1705.html:text/html;Lian et al_2017_Can Decentralized Algorithms Outperform Centralized Algorithms.pdf:/Users/dmitron/Zotero/storage/BP43SILI/Lian et al_2017_Can Decentralized Algorithms Outperform Centralized Algorithms.pdf:application/pdf},
	Journal = {arXiv:1705.09056 [cs, math, stat]},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Distributed, Parallel, and Cluster Computing},
	Month = may,
	Note = {arXiv: 1705.09056},
	Shorttitle = {Can {Decentralized} {Algorithms} {Outperform} {Centralized} {Algorithms}?},
	Title = {Can {Decentralized} {Algorithms} {Outperform} {Centralized} {Algorithms}? {A} {Case} {Study} for {Decentralized} {Parallel} {Stochastic} {Gradient} {Descent}},
	Url = {http://arxiv.org/abs/1705.09056},
	Urldate = {2018-05-09},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.09056}}

@article{park_hierarchical_2018,
	Abstract = {Coding for distributed computing supports low-latency computation by relieving the burden of straggling workers. While most existing works assume a simple master-worker model, we consider a hierarchical computational structure consisting of groups of workers, motivated by the need to reflect the architectures of real-world distributed computing systems. In this work, we propose a hierarchical coding scheme for this model, as well as analyze its decoding cost and expected computation time. Specifically, we first provide upper and lower bounds on the expected computing time of the proposed scheme. We also show that our scheme enables efficient parallel decoding, thus reducing decoding costs by orders of magnitude over non-hierarchical schemes. When considering both decoding cost and computing time, the proposed hierarchical coding is shown to outperform existing schemes in many practical scenarios.},
	Annote = {Comment: 7 pages, part of the paper is submitted to ISIT2018},
	Author = {Park, Hyegyeong and Lee, Kangwook and Sohn, Jy-yong and Suh, Changho and Moon, Jaekyun},
	Date-Added = {2018-05-12 23:14:23 +0000},
	Date-Modified = {2018-05-12 23:14:23 +0000},
	File = {arXiv\:1801.04686 PDF:/Users/dmitron/Zotero/storage/SYQV83R7/Park et al. - 2018 - Hierarchical Coding for Distributed Computing.pdf:application/pdf;arXiv.org Snapshot:/Users/dmitron/Zotero/storage/2RX63JAN/1801.html:text/html},
	Journal = {arXiv:1801.04686 [cs, math]},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Theory},
	Month = jan,
	Note = {arXiv: 1801.04686},
	Title = {Hierarchical {Coding} for {Distributed} {Computing}},
	Url = {http://arxiv.org/abs/1801.04686},
	Urldate = {2018-05-12},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1801.04686}}

@article{dong_hierarchical_2016,
	Abstract = {This paper introduces an effective processing framework nominated ICP (Image Cloud Processing) to powerfully cope with the data explosion in image processing field. While most previous researches focus on optimizing the image processing algorithms to gain higher efficiency, our work dedicates to providing a general framework for those image processing algorithms, which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale. The proposed ICP framework consists of two mechanisms, i.e. SICP (Static ICP) and DICP (Dynamic ICP). Specifically, SICP is aimed at processing the big image data pre-stored in the distributed system, while DICP is proposed for dynamic input. To accomplish SICP, two novel data representations named P-Image and Big-Image are designed to cooperate with MapReduce to achieve more optimized configuration and higher efficiency. DICP is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system. Representative results of comprehensive experiments on the challenging ImageNet dataset are selected to validate the capacity of our proposed ICP framework over the traditional state-of-the-art methods, both in time efficiency and quality of results.},
	Author = {Dong, Le and Lin, Zhiyu and Liang, Yan and He, Ling and Zhang, Ning and Chen, Qi and Cao, Xiaochun and lzquierdo, Ebroul},
	Date-Added = {2018-05-12 23:13:28 +0000},
	Date-Modified = {2018-05-12 23:13:28 +0000},
	Journal = {arXiv:1607.00577 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jul,
	Note = {arXiv: 1607.00577},
	Title = {A {Hierarchical} {Distributed} {Processing} {Framework} for {Big} {Image} {Data}},
	Url = {http://arxiv.org/abs/1607.00577},
	Urldate = {2018-05-12},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1607.00577}}

@article{georgopoulos_distributed_2014,
	Abstract = {We propose an algorithm to learn from distributed data on a network of arbitrarily connected machines without exchange of the data-points. Parts of the dataset are processed locally at each machine, and then the consensus communication algorithm is employed to consolidate the results. This iterative two stage process converges as if the entire dataset had been on a single machine. The principal contribution of this paper is the proof of convergence of the distributed learning process in the general case that the learning algorithm is a contraction. Moreover, we derive the distributed update equation of a feed-forward neural network with back-propagation for the purpose of verifying the theoretical results. We employ a toy classification example and a real world binary classification dataset.},
	Author = {Georgopoulos, Leonidas and Hasler, Martin},
	Date-Added = {2018-05-12 23:10:15 +0000},
	Date-Modified = {2018-05-12 23:10:15 +0000},
	Doi = {10.1016/j.neucom.2012.12.055},
	File = {ScienceDirect Snapshot:/Users/dmitron/Zotero/storage/RFCRWMN8/S0925231213003639.html:text/html},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {Consensus, Distributed machine learning, Gradient descent, Neural networks, Parallel machine learning, Peer-to-peer learning},
	Month = jan,
	Pages = {2--12},
	Title = {Distributed machine learning in networks by consensus},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231213003639},
	Urldate = {2018-05-12},
	Volume = {124},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231213003639},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2012.12.055}}

@article{xing_strategies_2016,
	Abstract = {The rise of big data has led to new demands for machine learning (ML) systems to learn complex models, with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics (such as high-dimensional latent features, intermediate representations, and decision functions) thereupon. In order to run ML algorithms at such scales, on a distributed cluster with tens to thousands of machines, it is often the case that significant engineering efforts are required---and one might fairly ask whether such engineering truly falls within the domain of ML research. Taking the view that ``big'' ML systems can benefit greatly from ML-rooted statistical and algorithmic insights---and that ML researchers should therefore not shy away from such systems design---we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of big ML systems and architectures, with the goal of understanding how to make them efficient, generally applicable, and supported with convergence and scaling guarantees. They concern four key questions that traditionally receive little attention in ML research: How can an ML program be distributed over a cluster? How can ML computation be bridged with inter-machine communication? How can such communication be performed? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems. .},
	Author = {Xing, Eric P. and Ho, Qirong and Xie, Pengtao and Wei, Dai},
	Date-Added = {2018-05-09 09:32:54 +0000},
	Date-Modified = {2018-05-09 09:32:54 +0000},
	Doi = {10.1016/J.ENG.2016.02.008},
	File = {Xing et al. - 2016 - Strategies and Principles of Distributed Machine L.pdf:/Users/dmitron/Zotero/storage/25BDTJFD/Xing et al. - 2016 - Strategies and Principles of Distributed Machine L.pdf:application/pdf},
	Issn = {20958099},
	Journal = {Engineering},
	Language = {en},
	Month = jun,
	Number = {2},
	Pages = {179--195},
	Title = {Strategies and {Principles} of {Distributed} {Machine} {Learning} on {Big} {Data}},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S2095809916309468},
	Urldate = {2018-05-03},
	Volume = {2},
	Year = {2016},
	Bdsk-Url-1 = {http://linkinghub.elsevier.com/retrieve/pii/S2095809916309468},
	Bdsk-Url-2 = {https://doi.org/10.1016/J.ENG.2016.02.008}}

@article{low_graphlab:_2014,
	Abstract = {Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.},
	Annote = {Comment: Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)},
	Author = {Low, Yucheng and Gonzalez, Joseph E. and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos E. and Hellerstein, Joseph},
	Date-Added = {2018-05-09 09:32:54 +0000},
	Date-Modified = {2018-05-09 09:32:54 +0000},
	Journal = {arXiv:1408.2041 [cs]},
	Keywords = {Computer Science - Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	Month = aug,
	Note = {arXiv: 1408.2041},
	Shorttitle = {{GraphLab}},
	Title = {{GraphLab}: {A} {New} {Framework} {For} {Parallel} {Machine} {Learning}},
	Url = {http://arxiv.org/abs/1408.2041},
	Urldate = {2018-05-04},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1408.2041}}

@article{younis_heed:_2004,
	Abstract = {Topology control in a sensor network balances load on sensor nodes and increases network scalability and lifetime. Clustering sensor nodes is an effective topology control approach. In this paper, we propose a novel distributed clustering approach for long-lived ad hoc sensor networks. Our proposed approach does not make any assumptions about the presence of infrastructure or about node capabilities, other than the availability of multiple power levels in sensor nodes. We present a protocol, HEED (Hybrid Energy-Efficient Distributed clustering), that periodically selects cluster heads according to a hybrid of the node residual energy and a secondary parameter, such as node proximity to its neighbors or node degree. HEED terminates in Oð1Þ iterations, incurs low message overhead, and achieves fairly uniform cluster head distribution across the network. We prove that, with appropriate bounds on node density and intracluster and intercluster transmission ranges, HEED can asymptotically almost surely guarantee connectivity of clustered networks. Simulation results demonstrate that our proposed approach is effective in prolonging the network lifetime and supporting scalable data aggregation.},
	Author = {Younis, O. and Fahmy, S.},
	Date-Added = {2018-05-09 09:32:54 +0000},
	Date-Modified = {2018-05-09 09:32:54 +0000},
	Doi = {10.1109/TMC.2004.41},
	File = {Younis and Fahmy - 2004 - HEED a hybrid, energy-efficient, distributed clus.pdf:/Users/dmitron/Zotero/storage/44WGS8K2/Younis and Fahmy - 2004 - HEED a hybrid, energy-efficient, distributed clus.pdf:application/pdf},
	Issn = {1536-1233},
	Journal = {IEEE Transactions on Mobile Computing},
	Language = {en},
	Month = oct,
	Number = {4},
	Pages = {366--379},
	Shorttitle = {{HEED}},
	Title = {{HEED}: a hybrid, energy-efficient, distributed clustering approach for ad hoc sensor networks},
	Url = {http://ieeexplore.ieee.org/document/1347100/},
	Urldate = {2018-05-03},
	Volume = {3},
	Year = {2004},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/1347100/},
	Bdsk-Url-2 = {https://doi.org/10.1109/TMC.2004.41}}

@article{tenney_detection_1981,
	Abstract = {The extension of classical detection theory to the case of distributed sensors is discussed, based on the theory of statistical hypothesis testing. The development is based on the formulation of a decentralized or team hypothesis testing problem. Theoretical results concerning the form of the optimal decision rule, examples, application to data fusion, and open problems are presented.},
	Author = {Tenney, R. R. and Sandell, N. R.},
	Date-Added = {2018-05-09 09:32:54 +0000},
	Date-Modified = {2018-05-09 09:32:54 +0000},
	Doi = {10.1109/TAES.1981.309178},
	File = {IEEE Xplore Abstract Record:/Users/dmitron/Zotero/storage/42JA3K92/4102537.html:text/html},
	Issn = {0018-9251},
	Journal = {IEEE Transactions on Aerospace and Electronic Systems},
	Keywords = {Cost function, Detectors, Radar detection, Radar tracking, Sensor fusion, Sensor systems, Signal processing, Sonar detection, Surveillance, Testing},
	Month = jul,
	Number = {4},
	Pages = {501--510},
	Title = {Detection with {Distributed} {Sensors}},
	Volume = {AES-17},
	Year = {1981},
	Bdsk-Url-1 = {https://doi.org/10.1109/TAES.1981.309178}}

@article{tenney1981detection,
	Author = {Tenney, Robert R and Sandell, Nils R},
	Date-Added = {2018-04-28 20:51:33 +0000},
	Date-Modified = {2018-04-28 20:51:33 +0000},
	Journal = {IEEE Transactions on Aerospace and Electronic systems},
	Number = {4},
	Pages = {501--510},
	Publisher = {IEEE},
	Title = {Detection with distributed sensors},
	Year = {1981}}

@article{amari1998statistical,
	Author = {Amari, S and others},
	Date-Added = {2018-04-27 01:33:57 +0000},
	Date-Modified = {2018-04-27 01:33:57 +0000},
	Journal = {IEEE Transactions on Information Theory},
	Number = {6},
	Pages = {2300--2324},
	Publisher = {IEEE},
	Title = {Statistical inference under multiterminal data compression},
	Volume = {44},
	Year = {1998}}

@article{ahlswede1986hypothesis,
	Author = {Ahlswede, Rudolf and Csisz{\'a}r, Imre},
	Date-Added = {2018-04-27 01:32:12 +0000},
	Date-Modified = {2018-04-27 01:32:12 +0000},
	Journal = {IEEE transactions on information theory},
	Number = {4},
	Pages = {533--542},
	Publisher = {IEEE},
	Title = {Hypothesis testing with communication constraints},
	Volume = {32},
	Year = {1986}}

@article{luo1993communication,
	Author = {Luo, Zhi-Quan and Tsitsiklis, John N},
	Date-Added = {2018-04-26 00:42:34 +0000},
	Date-Modified = {2018-04-26 00:42:34 +0000},
	Journal = {Journal of the ACM (JACM)},
	Number = {5},
	Pages = {1019--1047},
	Publisher = {ACM},
	Title = {On the communication complexity of distributed algebraic computation},
	Volume = {40},
	Year = {1993}}

@article{tsitsiklis1987communication,
	Author = {Tsitsiklis, John N and Luo, Zhi-Quan},
	Date-Added = {2018-04-26 00:41:18 +0000},
	Date-Modified = {2018-04-26 00:41:18 +0000},
	Journal = {Journal of Complexity},
	Number = {3},
	Pages = {231--243},
	Publisher = {Elsevier},
	Title = {Communication complexity of convex optimization},
	Volume = {3},
	Year = {1987}}

@book{ibragimov2013statistical,
	Author = {Ibragimov, Ilʹdar Abdulovi{\v{c}} and Has' Minskii, Rafail Zalmanovich},
	Date-Added = {2018-04-16 19:36:06 +0000},
	Date-Modified = {2018-04-16 19:36:06 +0000},
	Publisher = {Springer Science \& Business Media},
	Title = {Statistical estimation: asymptotic theory},
	Volume = {16},
	Year = {2013}}

@article{chamberland2003decentralized,
	Author = {Chamberland, J-F and Veeravalli, Venugopal V},
	Date-Added = {2018-04-06 19:11:59 +0000},
	Date-Modified = {2018-04-06 19:11:59 +0000},
	Journal = {IEEE Transactions on Signal Processing},
	Number = {2},
	Pages = {407--416},
	Publisher = {IEEE},
	Title = {Decentralized detection in sensor networks},
	Volume = {51},
	Year = {2003}}

@inproceedings{braverman2016communication,
	Author = {Braverman, Mark and Garg, Ankit and Ma, Tengyu and Nguyen, Huy L and Woodruff, David P},
	Booktitle = {Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
	Date-Added = {2018-04-06 19:11:02 +0000},
	Date-Modified = {2018-04-06 19:11:02 +0000},
	Organization = {ACM},
	Pages = {1011--1020},
	Title = {Communication lower bounds for statistical estimation problems via a distributed data processing inequality},
	Year = {2016}}

@inproceedings{xu2015converses,
	Author = {Xu, Aolin and Raginsky, Maxim},
	Booktitle = {Information Theory (ISIT), 2015 IEEE International Symposium on},
	Date-Added = {2018-04-06 19:09:25 +0000},
	Date-Modified = {2018-04-06 19:09:25 +0000},
	Organization = {IEEE},
	Pages = {2376--2380},
	Title = {Converses for distributed estimation via strong data processing inequalities},
	Year = {2015}}

@article{duchi2013distance,
	Author = {Duchi, John C and Wainwright, Martin J},
	Date-Added = {2018-04-06 18:53:03 +0000},
	Date-Modified = {2018-04-06 18:53:03 +0000},
	Journal = {arXiv preprint arXiv:1311.2669},
	Title = {Distance-based and continuum Fano inequalities with applications to statistical estimation},
	Year = {2013}}

@incollection{polyanskiy2017strong,
	Author = {Polyanskiy, Yury and Wu, Yihong},
	Booktitle = {Convexity and Concentration},
	Date-Added = {2018-04-06 18:47:47 +0000},
	Date-Modified = {2018-04-06 18:47:47 +0000},
	Pages = {211--249},
	Publisher = {Springer},
	Title = {Strong data-processing inequalities for channels and Bayesian networks},
	Year = {2017}}

@incollection{kushilevitz1997communication,
	Author = {Kushilevitz, Eyal},
	Booktitle = {Advances in Computers},
	Date-Added = {2018-04-06 18:43:28 +0000},
	Date-Modified = {2018-04-06 18:43:28 +0000},
	Pages = {331--360},
	Publisher = {Elsevier},
	Title = {Communication complexity},
	Volume = {44},
	Year = {1997}}

@inproceedings{bellet2015distributed,
	Author = {Bellet, Aur{\'e}lien and Liang, Yingyu and Garakani, Alireza Bagheri and Balcan, Maria-Florina and Sha, Fei},
	Booktitle = {Proceedings of the 2015 SIAM International Conference on Data Mining},
	Date-Added = {2018-04-06 18:38:06 +0000},
	Date-Modified = {2018-04-06 18:38:06 +0000},
	Organization = {SIAM},
	Pages = {478--486},
	Title = {A distributed frank-wolfe algorithm for communication-efficient sparse learning},
	Year = {2015}}

@inproceedings{zhang2015deep,
	Author = {Zhang, Sixin and Choromanska, Anna E and LeCun, Yann},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2018-04-06 18:36:26 +0000},
	Date-Modified = {2018-04-06 18:36:26 +0000},
	Pages = {685--693},
	Title = {Deep learning with elastic averaging SGD},
	Year = {2015}}

@inproceedings{arjevani2015communication,
	Author = {Arjevani, Yossi and Shamir, Ohad},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2018-04-06 18:35:20 +0000},
	Date-Modified = {2018-04-06 18:35:20 +0000},
	Pages = {1756--1764},
	Title = {Communication complexity of distributed convex learning and optimization},
	Year = {2015}}

@incollection{nguyen2013small,
	Author = {Nguyen, Hoi H and Vu, Van H},
	Booktitle = {Erd{\H{o}}s Centennial},
	Date-Added = {2018-04-06 18:28:39 +0000},
	Date-Modified = {2018-04-06 18:28:39 +0000},
	Pages = {409--463},
	Publisher = {Springer},
	Title = {Small ball probability, inverse theorems, and applications},
	Year = {2013}}

@article{valiant2016information,
	Author = {Valiant, Gregory and Valiant, Paul},
	Date-Added = {2018-04-06 18:27:54 +0000},
	Date-Modified = {2018-04-06 18:27:54 +0000},
	Journal = {arXiv preprint arXiv:1605.02646},
	Title = {Information theoretically secure databases},
	Year = {2016}}

@inproceedings{ciancio2006energy,
	Author = {Ciancio, Alexandre and Pattem, Sundeep and Ortega, Antonio and Krishnamachari, Bhaskar},
	Booktitle = {Proceedings of the 5th international conference on Information processing in sensor networks},
	Date-Added = {2018-04-06 18:27:30 +0000},
	Date-Modified = {2018-04-06 18:27:30 +0000},
	Organization = {ACM},
	Pages = {309--316},
	Title = {Energy-efficient data representation and routing for wireless sensor networks based on a distributed wavelet compression algorithm},
	Year = {2006}}

@article{duchi2014optimality,
	Author = {Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Zhang, Yuchen},
	Date-Added = {2018-04-05 19:07:47 +0000},
	Date-Modified = {2018-04-05 19:07:47 +0000},
	Journal = {arXiv preprint arXiv:1405.0782},
	Title = {Optimality guarantees for distributed statistical estimation},
	Year = {2014}}

@article{lopes2013sharp,
	Author = {Lopes, Miles E},
	Date-Added = {2018-04-05 19:05:19 +0000},
	Date-Modified = {2018-04-05 19:05:19 +0000},
	Journal = {arXiv preprint arXiv:1303.0727},
	Title = {A sharp bound on the computation-accuracy tradeoff for majority voting ensembles},
	Year = {2013}}

@article{nedic2017distributed,
	Author = {Nedi{\'c}, Angelia and Olshevsky, Alex and Uribe, C{\'e}sar A},
	Date-Added = {2018-04-05 19:02:47 +0000},
	Date-Modified = {2018-04-05 19:02:47 +0000},
	Journal = {arXiv preprint arXiv:1704.02718},
	Title = {Distributed Learning for Cooperative Inference},
	Year = {2017}}

@article{tsitsiklis1985complexity,
	Author = {Tsitsiklis, John and Athans, Michael},
	Date-Added = {2018-03-28 21:24:11 +0000},
	Date-Modified = {2018-03-28 21:24:11 +0000},
	Journal = {IEEE Transactions on Automatic Control},
	Number = {5},
	Pages = {440--446},
	Publisher = {IEEE},
	Title = {On the complexity of decentralized decision making and detection problems},
	Volume = {30},
	Year = {1985}}

@article{tsitsiklis1988decentralized,
	Author = {Tsitsiklis, John N},
	Date-Added = {2018-03-28 21:21:27 +0000},
	Date-Modified = {2018-03-28 21:21:27 +0000},
	Journal = {Mathematics of Control, Signals and Systems},
	Number = {2},
	Pages = {167--182},
	Publisher = {Springer},
	Title = {Decentralized detection by a large number of sensors},
	Volume = {1},
	Year = {1988}}

@article{tsitsiklis93,
	Author = {John Tsitsiklis},
	Date-Added = {2018-02-06 22:16:13 +0000},
	Date-Modified = {2018-02-06 22:16:40 +0000},
	Journal = {Advances in Statistical Signal Processing},
	Title = {Decentralized Detection},
	Year = {1993}}

@article{ohad2014,
	Author = {Ohad Shamir},
	Date-Added = {2018-02-06 06:11:36 +0000},
	Date-Modified = {2018-02-06 06:17:20 +0000},
	Journal = {NIPS Conference},
	Title = {Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation},
	Year = {2014}}
